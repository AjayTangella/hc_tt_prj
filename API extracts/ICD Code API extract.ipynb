{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3922adcb-e655-4105-bb69-dc79fc9893d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date, lit\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, BooleanType\n",
    "\n",
    "\n",
    "\n",
    "client_id = 'c1b31e75-de89-4e9e-9738-87a4779e98e9_b7b0ac50-33a3-4ba5-abc2-4f4b1050ab15'\n",
    "client_secret = 'UtTb8EIodKWT2g3BoIkgGsNbMcv0ClZKJbEij4ySzTs='\n",
    "base_url = 'https://id.who.int/icd/'\n",
    "current_date=datetime.now().date()\n",
    "\n",
    "auth_url = 'https://icdaccessmanagement.who.int/connect/token'\n",
    "auth_response = requests.post(auth_url, data={\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "    'grant_type': 'client_credentials'\n",
    "})\n",
    "\n",
    "if auth_response.status_code == 200:\n",
    "    access_token = auth_response.json().get('access_token')\n",
    "else:\n",
    "    raise Exception(f\"Failed to obtain access token: {auth_response.status_code} - {auth_response.text}\")\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {access_token}',\n",
    "    'API-Version': 'v2',  # Add the API-Version header\n",
    "    'Accept-Language': 'en',\n",
    "}\n",
    "\n",
    "def fetch_icd_codes(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch data: {response.status_code} - {response.text}\")\n",
    "\n",
    "def extract_codes(url):\n",
    "    data = fetch_icd_codes(url)\n",
    "    codes = []\n",
    "    if 'child' in data:\n",
    "        for child_url in data['child']:\n",
    "            #print(f'defining child url: {child_url}')\n",
    "            codes.extend(extract_codes(child_url))\n",
    "    else:\n",
    "        if 'code' in data and 'title' in data:\n",
    "            # print(data['code'],data['title']['@value'])\n",
    "            codes.append({\n",
    "                'icd_code': data['code'],\n",
    "                'icd_code_type': 'ICD-10',\n",
    "                'code_description': data['title']['@value'],\n",
    "                'inserted_date': current_date,\n",
    "                'updated_date': current_date,\n",
    "                'is_current_flag': True\n",
    "            })\n",
    "    return codes\n",
    "\n",
    "# Start from the root URL\n",
    "root_url = 'https://id.who.int/icd/release/10/2019/A00-A09'\n",
    "icd_codes = extract_codes(root_url)\n",
    "\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"icd_code\", StringType(), True),\n",
    "    StructField(\"icd_code_type\", StringType(), True),\n",
    "    StructField(\"code_description\", StringType(), True),\n",
    "    StructField(\"inserted_date\", DateType(), True),\n",
    "    StructField(\"updated_date\", DateType(), True),\n",
    "    StructField(\"is_current_flag\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "#Create a DataFrame using the defined schema\n",
    "print(icd_codes)\n",
    "df = spark.createDataFrame(icd_codes, schema=schema)\n",
    "df.write.format(\"parquet\").mode(\"append\").save(\"/mnt/bronze/icd_codes/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ec37c3-1465-4d97-b798-f0a17cf00bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Databricks notebook source\n",
    "# # Import necessary libraries\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# from pyspark.sql import SparkSession\n",
    "# from requests.adapters import HTTPAdapter\n",
    "# from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CPTCodeAPI\").getOrCreate()\n",
    "\n",
    "# # API endpoint and parameters\n",
    "# api_url = \"https://api.cms.gov/ppl-api/codes\"\n",
    "# headers = {\n",
    "#     \"Accept\": \"application/json\",\n",
    "#     \"API-Key\": \"YOUR_API_KEY\"  # Replace with your actual API key\n",
    "# }\n",
    "\n",
    "# # Set up retry strategy\n",
    "# retry_strategy = Retry(\n",
    "#     total=3,\n",
    "#     status_forcelist=[429, 500, 502, 503, 504],\n",
    "#     method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "# )\n",
    "# adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "# http = requests.Session()\n",
    "# http.mount(\"https://\", adapter)\n",
    "\n",
    "# # Fetch data from API with retry\n",
    "# try:\n",
    "#     response = http.get(api_url, headers=headers)\n",
    "#     response.raise_for_status()  # Raise an error for bad status codes\n",
    "#     data = response.json()\n",
    "#     print(\"Data fetched successfully\")\n",
    "# except requests.ConnectionError as e:\n",
    "#     print(\"Connection failed after retries:\", e)\n",
    "# except requests.HTTPError as e:\n",
    "#     print(\"HTTP error occurred:\", e)\n",
    "# except Exception as e:\n",
    "#     print(\"An error occurred:\", e)\n",
    "\n",
    "# # Convert to DataFrame if data is fetched successfully\n",
    "# if 'data' in locals():\n",
    "#     df = pd.DataFrame(data['results'])\n",
    "    \n",
    "#     # Convert to Spark DataFrame\n",
    "#     spark_df = spark.createDataFrame(df)\n",
    "    \n",
    "#     # Display the Spark DataFrame\n",
    "#     display(spark_df)\n",
    "    \n",
    "#     # Write to Delta table\n",
    "#     spark_df.write.format(\"delta\").save(\"/mnt/delta/cpt_codes\")\n",
    "# else:\n",
    "#     print(\"No data to process\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1b8524-fd54-49f4-9f41-724bf8b0d766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4429486847610369,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "ICD Code API extract",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
